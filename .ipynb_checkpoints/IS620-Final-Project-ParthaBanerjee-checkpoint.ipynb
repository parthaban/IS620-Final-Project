{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IS620 Web Analytics Final Project\n",
    "\n",
    "\n",
    "## A Better Sentiment Analysis System\n",
    "\n",
    "\n",
    "#### Author: Partha Banerjee, CUNY MSDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">** Final Project Requirement **\n",
    ">\n",
    "> \"Your project should incorporate one or both of the two main themes of this course: network analysis and text processing. You need to show all of your work in a coherent workflow, and in a reproducible format, such as an IPython Notebook or an R Markdown document. If you are building a model or models, explain how you evaluate the “goodness” of the chosen model and parameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is to create a *model* which will be able to analyze the sentiment more accurately. The target is to train the model to predict “not cool” as negative instead of positive as predicted by majority of the models due to the positive word “cool”.\n",
    "\n",
    "As you all know, sentiment analysis helps modern business many ways - a prospect buyers can use the sentiments of other buyers to decide about the product (s)he is planning to buy, producers can plan about their product lines based upon buyers sentiment, producers can take corrective measures to address negative sentiment about their product, marketers can use this for their research and recommendation etc. Social media like twitter, facebook play an important role to spread this sentiment very quickly.\n",
    "\n",
    "In this project, I will first build the model and test its accuracy. Then I will get twitter data based upon user given hashtag and check their sentiment using the model. This project will comply the Project Requirements as follows:\n",
    "\n",
    "* This is a text processing project.\n",
    "* It is reproducible, written in IPython. I will share everything except my twitter credentials.\n",
    "* I will also include the twitter data in .TXT format. This is to support the situation if twitter is unavailable (due to credentials etc.)\n",
    "* Compare among different conditions and model's behavior.\n",
    "* Finally, to deliver a model which can identify \"not cool\" as negative, not positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While working on this project, I got a good deal of help from the text book and the following sites:\n",
    "\n",
    "* <a href=\"http://ravikiranj.net/posts/2012/code/how-build-twitter-sentiment-analyzer/\">how to build a twitter sentiment analyzer?</a>\n",
    "* <a href=\"http://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/\">Text Classification For Sentiment Analysis – Stopwords And Collocations</a>\n",
    "\n",
    "Now let's start without spending much time talking things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment\n",
    "\n",
    "Let us start with setting up the environment with all necessary libraries in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, math, collections, itertools, os\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Evaluator\n",
    "\n",
    "As we have learnt in our course, “features” are an important piece in sentiment analysis. Whatever someone is analyzing is an attempt to correlate the subject to the labels. In this code, the features will be the words in each review. \n",
    "\n",
    "* The evaluate_features() function will splits up the review by line and then builds a two variables posFeatures and negFeatures containing the output of feature selection mechanism with ‘Positive’ or ‘Negative’ appended to it based upon the review file used.\n",
    "* Then it separates the data into training and testing datasets for a Naive Bayes classifier which is the type of classifier I used.\n",
    "* Now I train my classifier using NLTK's NaiveBayesClassifier and using my training dataset.\n",
    "* Next test the classifier using test dataset by initiating two more variables - referenceSets and testSets. referenceSets will contain the actual values for the testing data and testSets will contain the predicted output.\n",
    "* Each one of the testFeatures (the reviews that need testing) iterated through three things: an arbitrary identifier 'i', the features (or words) in the review, and the actual label (‘Positive’ or ‘Negative’).\n",
    "* Finally display statistics of the model - accuracy, precision and recall. Referring NLTK\n",
    "\n",
    "    * Accuracy: Given a list of reference values and a corresponding list of test values, return the fraction of corresponding values that are equal.\n",
    "    * Precision: Given a set of reference values and a set of test values, return the fraction of test values that appear in the reference set.\n",
    "    * Recall: Given a set of reference values and a set of test values, return the fraction of reference values that appear in the test set.\n",
    "    * show_most_informative_features: Display top 25 features that were most helpful to the classifier in determining whether a review was positive or negative.\n",
    "\n",
    "* And then return the classifier model.\n",
    "\n",
    "** Data Source **\n",
    "\n",
    "For building feature corpus, I have used *<a href=\"http://www.cs.cornell.edu/people/pabo/movie-review-data/\">sentence polarity dataset v1.0</a>* having 5,331 positive and 5,331 negative processed sentences / snippets introduced by Cornell professor Bo Pang in Pang/Lee ACL 2005 which was released in July 2005. Though this data collected on movie review, but we can still use this dataset to use for our purpose. The data is available freely for use after citation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_features(feature_fun):\n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "    # http://stackoverflow.com/questions/367155/splitting-a-string-into-words-and-punctuation\n",
    "    # Breaks up the sentences into lists of individual words (as selected by the \n",
    "    # input mechanism) and appends 'pos' or 'neg' after each list\n",
    "    with open('./data/rt-polarity.pos', 'r') as posSentences:\n",
    "        for i in posSentences:\n",
    "            posWords = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "            posWords = [feature_fun(posWords), 'Positive']\n",
    "            posFeatures.append(posWords)\n",
    "    with open('./data/rt-polarity.neg', 'r') as negSentences:\n",
    "        for i in negSentences:\n",
    "            negWords = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "            negWords = [feature_fun(negWords), 'Negative']\n",
    "            negFeatures.append(negWords)\n",
    "\n",
    "    # Now we need to split the data into 80:20 ratio as training and \n",
    "    # testing data for a Naive Bayes classifier.\n",
    "    posCutoff = int(math.floor(len(posFeatures)*0.8))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*0.8))\n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "\n",
    "    # Trains a Naive Bayes Classifier using NLTK\n",
    "    classifier = NaiveBayesClassifier.train(trainFeatures)\n",
    "\n",
    "    # Initiates referenceSets and testSets\n",
    "    referenceSets = collections.defaultdict(set)\n",
    "    testSets = collections.defaultdict(set)\n",
    "\n",
    "    # Puts correctly labeled sentences in referenceSets and the \n",
    "    # predictively labeled version in testsets\n",
    "    for i, (features, label) in enumerate(testFeatures):\n",
    "        referenceSets[label].add(i)\n",
    "        predicted = classifier.classify(features)\n",
    "        testSets[predicted].add(i)\n",
    "\n",
    "    # Display model statistics\n",
    "    print 'Train on {:,} instances, Test on {:,} instances'.format( \\\n",
    "                            len(trainFeatures), len(testFeatures))\n",
    "    print 'Accuracy:', nltk.classify.util.accuracy(classifier, testFeatures)\n",
    "    print 'Positive precision:', nltk.metrics.precision(referenceSets['Positive'], \\\n",
    "                            testSets['Positive'])\n",
    "    print 'Positive recall:', nltk.metrics.recall(referenceSets['Positive'], \\\n",
    "                            testSets['Positive'])\n",
    "    print 'Negative precision:', nltk.metrics.precision(referenceSets['Negative'], \\\n",
    "                            testSets['Negative'])\n",
    "    print 'Negative recall:', nltk.metrics.recall(referenceSets['Negative'], \\\n",
    "                            testSets['Negative'])\n",
    "    print\n",
    "    classifier.show_most_informative_features(25)\n",
    "    \n",
    "    # Finally return the model for future use\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Baseline Bag of Words Feature Extraction**\n",
    "\n",
    "Now let us start with all the words in each review. The function make_complete_dict() builds a dictionary object that has each of the words in the review followed by ‘True’. Then run the test using the dictionary object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8,528 instances, Test on 2,134 instances\n",
      "Accuracy: 0.778819119025\n",
      "Positive precision: 0.787996127783\n",
      "Positive recall: 0.762886597938\n",
      "Negative precision: 0.770208900999\n",
      "Negative recall: 0.794751640112\n",
      "\n",
      "Most Informative Features\n",
      "              engrossing = True           Positi : Negati =     18.3 : 1.0\n",
      "                   flaws = True           Positi : Negati =     13.7 : 1.0\n",
      "                mediocre = True           Negati : Positi =     13.7 : 1.0\n",
      "               absorbing = True           Positi : Negati =     13.0 : 1.0\n",
      "                 generic = True           Negati : Positi =     13.0 : 1.0\n",
      "                  boring = True           Negati : Positi =     12.4 : 1.0\n",
      "              refreshing = True           Positi : Negati =     12.3 : 1.0\n",
      "               inventive = True           Positi : Negati =     12.3 : 1.0\n",
      "                    flat = True           Negati : Positi =     11.8 : 1.0\n",
      "                    lame = True           Negati : Positi =     11.7 : 1.0\n",
      "                 triumph = True           Positi : Negati =     11.7 : 1.0\n",
      "            refreshingly = True           Positi : Negati =     11.7 : 1.0\n",
      "                 routine = True           Negati : Positi =     11.0 : 1.0\n",
      "              disturbing = True           Positi : Negati =     11.0 : 1.0\n",
      "               affecting = True           Positi : Negati =     11.0 : 1.0\n",
      "                 culture = True           Positi : Negati =     10.7 : 1.0\n",
      "                touching = True           Positi : Negati =     10.7 : 1.0\n",
      "                    dull = True           Negati : Positi =     10.6 : 1.0\n",
      "                  stupid = True           Negati : Positi =     10.6 : 1.0\n",
      "               wonderful = True           Positi : Negati =     10.6 : 1.0\n",
      "             mesmerizing = True           Positi : Negati =     10.3 : 1.0\n",
      "                   stale = True           Negati : Positi =     10.3 : 1.0\n",
      "                chilling = True           Positi : Negati =     10.3 : 1.0\n",
      "                mindless = True           Negati : Positi =     10.3 : 1.0\n",
      "                provides = True           Positi : Negati =     10.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "def make_complete_dict(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "monogramClassifier = evaluate_features(make_complete_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy 77.88% is good, but we will try to find a better accuracy. The precisions and recalls are also pretty close to each other indicating that it is classifying everything fairly evenly.\n",
    "\n",
    "From most informative features we see the chance of a word being positive vs. negative. For example, if ‘engrossing’ is in a review, there’s a 18:1 chance the review is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopword Filtering**\n",
    "\n",
    "Let us now remove stopwords and see how the model behaves using accuracy parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8,528 instances, Test on 2,134 instances\n",
      "Accuracy: 0.77038425492\n",
      "Positive precision: 0.766882516189\n",
      "Positive recall: 0.77694470478\n",
      "Negative precision: 0.773979107312\n",
      "Negative recall: 0.763823805061\n",
      "\n",
      "Most Informative Features\n",
      "              engrossing = True           Positi : Negati =     18.3 : 1.0\n",
      "                   flaws = True           Positi : Negati =     13.7 : 1.0\n",
      "                mediocre = True           Negati : Positi =     13.7 : 1.0\n",
      "               absorbing = True           Positi : Negati =     13.0 : 1.0\n",
      "                 generic = True           Negati : Positi =     13.0 : 1.0\n",
      "                  boring = True           Negati : Positi =     12.4 : 1.0\n",
      "              refreshing = True           Positi : Negati =     12.3 : 1.0\n",
      "               inventive = True           Positi : Negati =     12.3 : 1.0\n",
      "                    flat = True           Negati : Positi =     11.8 : 1.0\n",
      "                    lame = True           Negati : Positi =     11.7 : 1.0\n",
      "                 triumph = True           Positi : Negati =     11.7 : 1.0\n",
      "            refreshingly = True           Positi : Negati =     11.7 : 1.0\n",
      "                 routine = True           Negati : Positi =     11.0 : 1.0\n",
      "              disturbing = True           Positi : Negati =     11.0 : 1.0\n",
      "               affecting = True           Positi : Negati =     11.0 : 1.0\n",
      "                 culture = True           Positi : Negati =     10.7 : 1.0\n",
      "                touching = True           Positi : Negati =     10.7 : 1.0\n",
      "                    dull = True           Negati : Positi =     10.6 : 1.0\n",
      "                  stupid = True           Negati : Positi =     10.6 : 1.0\n",
      "               wonderful = True           Positi : Negati =     10.6 : 1.0\n",
      "             mesmerizing = True           Positi : Negati =     10.3 : 1.0\n",
      "                   stale = True           Negati : Positi =     10.3 : 1.0\n",
      "                chilling = True           Positi : Negati =     10.3 : 1.0\n",
      "                mindless = True           Negati : Positi =     10.3 : 1.0\n",
      "                provides = True           Positi : Negati =     10.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    " \n",
    "def stopword_filtered_word_feats(words):\n",
    "    return dict([(word, True) for word in words if word not in stopset])\n",
    " \n",
    "nostopwordClassifier = evaluate_features(stopword_filtered_word_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy has gone down from 77.88% to 77.03%. Also negative recall has gone down a bit. This is an indication that stopwords add information to sentiment analysis classification. So, we should not remove stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigram Collection**\n",
    "\n",
    "Let us now include bigrams to see the accuracy parameters. We will use NLTK library bigram features for this. The BigramCollocationFinder maintains 2 internal FreqDists -- one for individual word frequencies, another for bigram frequencies. Once it has these frequency distributions, it can score individual bigrams using a scoring function provided by BigramAssocMeasures, such chi-square. These scoring functions measure the collocation correlation of 2 words, basically whether the bigram occurs about as frequently as each individual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8,528 instances, Test on 2,134 instances\n",
      "Accuracy: 0.787722586692\n",
      "Positive precision: 0.792380952381\n",
      "Positive recall: 0.779756326148\n",
      "Negative precision: 0.783210332103\n",
      "Negative recall: 0.795688847235\n",
      "\n",
      "Most Informative Features\n",
      "              engrossing = True           Positi : Negati =     18.3 : 1.0\n",
      "                mediocre = True           Negati : Positi =     13.7 : 1.0\n",
      "          (',', 'funny') = True           Positi : Negati =     13.7 : 1.0\n",
      "                   flaws = True           Positi : Negati =     13.7 : 1.0\n",
      "           ('dull', ',') = True           Negati : Positi =     13.7 : 1.0\n",
      "               absorbing = True           Positi : Negati =     13.0 : 1.0\n",
      "          ('to', 'care') = True           Negati : Positi =     13.0 : 1.0\n",
      "           ('up', 'for') = True           Positi : Negati =     13.0 : 1.0\n",
      "                 generic = True           Negati : Positi =     13.0 : 1.0\n",
      "   ('examination', 'of') = True           Positi : Negati =     13.0 : 1.0\n",
      "                  boring = True           Negati : Positi =     12.4 : 1.0\n",
      "              refreshing = True           Positi : Negati =     12.3 : 1.0\n",
      "               inventive = True           Positi : Negati =     12.3 : 1.0\n",
      "        ('with', 'such') = True           Positi : Negati =     12.3 : 1.0\n",
      "                    flat = True           Negati : Positi =     11.8 : 1.0\n",
      "                    lame = True           Negati : Positi =     11.7 : 1.0\n",
      "                 triumph = True           Positi : Negati =     11.7 : 1.0\n",
      "        ('the', 'title') = True           Negati : Positi =     11.7 : 1.0\n",
      "            refreshingly = True           Positi : Negati =     11.7 : 1.0\n",
      "          ('about', '.') = True           Negati : Positi =     11.0 : 1.0\n",
      "                 routine = True           Negati : Positi =     11.0 : 1.0\n",
      "              disturbing = True           Positi : Negati =     11.0 : 1.0\n",
      "               affecting = True           Positi : Negati =     11.0 : 1.0\n",
      "                touching = True           Positi : Negati =     10.7 : 1.0\n",
      "                 culture = True           Positi : Negati =     10.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "def bigram_word_features(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    " \n",
    "bigramClassifier = evaluate_features(bigram_word_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried several values for finding the optimal value for number of best bigrams to produce the result, but gotten the same value for the range between 50 and 500. I have settled using the 200 best bigrams from each file.\n",
    "\n",
    "From the above, we can see that accuracy is now up from 77.88% to 78.77%. Also both positive and negative precision and recall values have increased. So we can conclude that including bigrams can increase classifier effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get into the Business\n",
    "\n",
    "Now after seeing the benefits of bigram, let us start using our models to find the sentiments of tweets. But before extracting tweeter data, let me use a sample test data to see the results using the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To prodice the sentiment of review comment using given model\n",
    "def predictSentiment(tweet, classifier):\n",
    "    twt = []\n",
    "    for key, value in tweet.iteritems():\n",
    "        twt.append(value)\n",
    "        \n",
    "    twFeatures = []\n",
    "    for i in twt[0]:\n",
    "        twWords = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "        twWords = [bigram_word_features(twWords), 'tbd']\n",
    "        twFeatures.append(twWords)\n",
    "\n",
    "    for i, (features, label) in enumerate(twFeatures):\n",
    "        predicted = classifier.classify(features)\n",
    "        print \"{} - {}\".format(twt[0][i], predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a sample test data\n",
    "test = {0: [\"you look so cool\", \\\n",
    "            \"he looks not so cool\", \\\n",
    "            \"so cool\", \\\n",
    "            \"not so cool\", \\\n",
    "            \"so great\", \\\n",
    "            \"not so great\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let me first use my model having no stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you look so cool - Positive\n",
      "he looks not so cool - Positive\n",
      "so cool - Positive\n",
      "not so cool - Positive\n",
      "so great - Positive\n",
      "not so great - Positive\n"
     ]
    }
   ],
   "source": [
    "predictSentiment(test, nostopwordClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it failed to judge the sentiment correctly which we already thought based upon the statistics analysis above.\n",
    "\n",
    "Now let us see how the model behave using bags of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you look so cool - Positive\n",
      "he looks not so cool - Negative\n",
      "so cool - Positive\n",
      "not so cool - Negative\n",
      "so great - Positive\n",
      "not so great - Positive\n"
     ]
    }
   ],
   "source": [
    "predictSentiment(test, monogramClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better result, though it has failed to predict \"not so great\" correctly.\n",
    "\n",
    "Finally let us check how it behaves using the model with bigram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you look so cool - Positive\n",
      "he looks not so cool - Negative\n",
      "so cool - Positive\n",
      "not so cool - Negative\n",
      "so great - Positive\n",
      "not so great - Negative\n"
     ]
    }
   ],
   "source": [
    "predictSentiment(test, bigramClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And bingo, it worked. The model is able to predict the sentiment more accurately.\n",
    "\n",
    "Now let us do the final part of this project - get twitter data and run the prediction using bigram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data from Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to have data for analyzing its sentiment and I extract the data from twitter. Data extraction is based upon the key word and time period. I have chosen them just for demonstrating my project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse, urllib, urllib2, json, random\n",
    "import os, oauth2, datetime, re\n",
    "from datetime import timedelta\n",
    "\n",
    "class TwitterData:\n",
    "    def __init__(self):\n",
    "        self.currDate = datetime.datetime.now()\n",
    "        self.weekDates = []\n",
    "        self.weekDates.append(self.currDate.strftime(\"%Y-%m-%d\"))\n",
    "        for i in range(1,7):\n",
    "            dateDiff = timedelta(days=-i)\n",
    "            newDate = self.currDate + dateDiff\n",
    "            self.weekDates.append(newDate.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    def getTwitterData(self, keyword, time):\n",
    "        self.weekTweets = {}\n",
    "        if(time == 'lastweek'):\n",
    "            for i in range(0,6):\n",
    "                params = {'since': self.weekDates[i+1], 'until': \\\n",
    "                          self.weekDates[i]}\n",
    "                self.weekTweets[i] = self.getData(keyword, params)\n",
    "        elif(time == 'today'):\n",
    "            for i in range(0,1):\n",
    "                params = {'since': self.weekDates[i+1], 'until': \\\n",
    "                          self.weekDates[i]}\n",
    "                self.weekTweets[i] = self.getData(keyword, params)\n",
    "        return self.weekTweets\n",
    "    \n",
    "    def parse_config(self):\n",
    "        config = {}\n",
    "        if os.path.exists('config.json'):\n",
    "            with open('config.json') as f:\n",
    "                config.update(json.load(f))\n",
    "        return config\n",
    "\n",
    "    def oauth_req(self, url, http_method=\"GET\", post_body=None,\n",
    "                  http_headers=None):\n",
    "        config = self.parse_config()\n",
    "        consumer = oauth2.Consumer(key=config.get('consumer_key'), \\\n",
    "                                   secret=config.get('consumer_secret'))\n",
    "        token = oauth2.Token(key=config.get('access_token'), \\\n",
    "                             secret=config.get('access_token_secret'))\n",
    "        client = oauth2.Client(consumer, token)\n",
    "\n",
    "        resp, content = client.request(\n",
    "            url,\n",
    "            method=http_method,\n",
    "            body=post_body or '',\n",
    "            headers=http_headers\n",
    "        )\n",
    "        return content\n",
    "    \n",
    "    def getData(self, keyword, params = {}):\n",
    "        maxTweets = 50\n",
    "        url = 'https://api.twitter.com/1.1/search/tweets.json?'    \n",
    "        data = {'q': keyword, 'lang': 'en', 'result_type': 'recent', \\\n",
    "                'count': maxTweets, 'include_entities': 0}\n",
    "\n",
    "        if params:\n",
    "            for key, value in params.iteritems():\n",
    "                data[key] = value\n",
    "        \n",
    "        url += urllib.urlencode(data)\n",
    "        \n",
    "        response = self.oauth_req(url)\n",
    "        jsonData = json.loads(response)\n",
    "        \n",
    "        tweets = []\n",
    "        if 'errors' in jsonData:\n",
    "            print \"API Error\"\n",
    "            print jsonData['errors']\n",
    "        else:\n",
    "            for item in jsonData['statuses']:\n",
    "                tweets.append(item['text'])\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get keyword and define time to extract data from twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter hash tag (with #) you want to retrieve? Trump\n"
     ]
    }
   ],
   "source": [
    "keyword = raw_input('Enter hash tag (with #) you want to retrieve? ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the keyword is a hashtag and is in lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered keyword: #trump\n"
     ]
    }
   ],
   "source": [
    "if keyword[:1] != \"#\":\n",
    "    keyword = \"#\" + keyword\n",
    "keyword = keyword.lower()\n",
    "print \"Entered keyword:\", keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#keyword = '#Trump'\n",
    "time = 'today'\n",
    "twitterData = TwitterData()\n",
    "tweets = twitterData.getTwitterData(keyword, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.dump(tweets, open(\"./data/tweets.txt\",'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleanup**\n",
    "\n",
    "* Convert the tweets to lower case\n",
    "* Remove Unicode\n",
    "* Remove URLs\n",
    "* Remove @usernames\n",
    "* Remove additional white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processTweet(tweet):\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Remove unicode\n",
    "    tweet = tweet.encode('ascii','ignore')\n",
    "    #Remove www.* or https?://*\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',tweet)\n",
    "    tweet = tweet.replace(\"https://\",\"\")\n",
    "    tweet = tweet.replace(\"https:\",\"\")\n",
    "    #Remove @username\n",
    "    tweet = re.sub('@[^\\s]+',' ',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    #Finally remove rt from the begining \n",
    "    if tweet[:3] == \"rt \":\n",
    "        tweet = tweet[3:]\n",
    "    return tweet\n",
    "\n",
    "processedTweet = {k: map(processTweet, v) for k, v in tweets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(processedTweet, open(\"./data/processedTweets.txt\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - Negative\n",
      " isn't it time to remove donald trump from the hall of fame considering the racist rule and all? wwe trump - Negative\n",
      " donald trump live in iowa buildthewall trumpkin trumptrain trumptoday trumpmiami makeamericagreatagain trump - Positive\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - Negative\n",
      "nice try cont. new leftist anti-trump narrative they \"care\" abt gop \"dilemma\". spare the false \"mournful state\" pity. cnn trump2016 - Positive\n",
      "rt pat buchanan's message to america! get behind trump!!! makeamericagreatagain  - Positive\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - Negative\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - Negative\n",
      "intelligent analysis of trump + prospects for 2016 repub convention by for 4 min.  - Negative\n",
      "oh boy: hoist on his own petard!! this should be fun! anonymous just declared war against donald trump  - Negative\n",
      "even die-hard hillaryclinton supporters can't think it's ok for trump to get more coverage than all the democrats combined! democracy - Negative\n",
      "left targets trump by finding nutty supporters--let's meet some obama &amp; hillary supporters  - Negative\n",
      "someone didn't like a trump campaign advertisement in cody, wyoming  - Negative\n",
      "anonyops: optrump: petition to ban trump from the uk now has over half a million signatures!  - Negative\n",
      "to hillary clinton, donald trump is no longer a laughing matter trump will beat her pants suit  - Negative\n",
      "sharpton has done zero for blacks and only enriched himself. now he's enraged black ministers are warming to trump  - Negative\n",
      "left targets trump by finding nutty supporters--let's meet some obama &amp; hillary supporters  - Negative\n",
      "i typically vote for \"none of the above\", &amp; antiestablishment &amp; agenda. &amp; right now trump is it! imvotingtrump2016 ht - Positive\n",
      "left targets trump by finding nutty supporters--let's meet some obama &amp; hillary supporters  - Negative\n",
      "why dems and republicans are slamming trump?  - Negative\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - Negative\n",
      "why dems and republicans are slamming trump?  - Negative\n",
      "we gave housesenate now they conspire to sabotage the will of the people's vote when trump wins the nomination?  - Negative\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - Negative\n",
      "that should make you stop talking abt trump and focus on true evil  - Negative\n",
      "palin blasts msm herd, naive pundits for opposing trumps temp muslim ban radicalislam  - Negative\n",
      "trump doesn't mind rich muslims...  - Negative\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - Negative\n",
      "or maybe twitter should make this punk famous trump  - Positive\n",
      "this is what i have felt about how media is handling trump anti muslim rants. bad news!  - Negative\n",
      "this is unexpected. moderate texas imam says he supports trump's proposal; then forced to resign from his mosque. http - Negative\n",
      "palin blasts msm herd, naive pundits for opposing trumps temp muslim ban radicalislam  - Negative\n",
      "trump remarks 2 banmuslims r causin white zionazis 2 attack us in usa may a white zionazi return the favor on trump bantrump 2 speak - Negative\n",
      "trump donaldchimp (apology to chimps)  - Positive\n",
      " donald trump live in iowa trump trumpdemands trump2016 women4trump needtrumpnow unitedwestand - Positive\n",
      "i have hope now for this country! hollywood/media continue to lie about trump yet the people aren't listening anymore!  - Negative\n",
      "why i love america: because men like this make the world a better place, and this is who we are. trump cruz  - Positive\n",
      " so any trump supporter could have this little coward arrested for making a deaththreat, correct?  - Negative\n",
      "specialreport the gope is way off like always! trump is going to win.......  - Negative\n",
      "i've never been so disgraced &amp; disappointed w our govt &amp; our worlds in crisis &amp; all they care abt is taking dwn t - Negative\n",
      "left targets trump by finding nutty supporters--let's meet some obama &amp; hillary supporters  - Negative\n",
      "is the republican convention going to be open carry? gop trump gopwar - Negative\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - Negative\n",
      "left targets trump by finding nutty supporters--let's meet some obama &amp; hillary supporters  - Negative\n",
      "franklin graham behind trump on muslim ban trumpisright bit me gop i support tedcruz islamistheproblem  - Negative\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - Negative\n",
      "this is unexpected. moderate texas imam says he supports trump's proposal; then forced to resign from his mosque. http - Negative\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - Negative\n",
      "i guess that police order in nh that just endorsed trump is gonna be vanguard of his brownshirts? disgusting!!.  - Negative\n",
      "mark shields on ted cruz=donald trump + 60 iq points + better haircut+better academic credentials - Negative\n"
     ]
    }
   ],
   "source": [
    "predictSentiment(processedTweet, bigramClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the bigram hypothesis is correct, including significant bigrams can increase classifier effectiveness. While this produces the result of my stated goal, I am sure it needs further improvement since the accuracy is still at 78.77%. To improve, I definitely want to get a relevant source of positive and negative data sources for training my model. Also I want to spend some more time to find the optimal level of bigram values to use (In my case, I have taken n=200). I am also interested to see the use of trigram, but this is a future plan.\n",
    "\n",
    "Overall, I am satisfied that I have been able to produce the result I was looking for. This concludes my project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
