{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Better Sentiment Analysis System\n",
    "\n",
    "#### IS620 Final Project\n",
    "\n",
    "##### Author: Partha Banerjee, CUNY MSDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is to create a model which will be able to analyze the sentiment more accurately. The target is to train the model to predict “not cool” as negative instead of positive as predicted by majority of the models due to the positive word “cool”.\n",
    "\n",
    "Sentiment analysis helps modern business many ways - a prospect buyers can use the sentiments of other buyers to decide about the product (s)he is planning to buy, producers can plan about their product lines based upon buyers sentiment, producers can take corrective measures to address negative sentiment about their product, marketers can use this for their research and recommendation the best etc. Social media like twiteer, facebook play an important role to spread this sentiment very quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While working on this project, I got a good deal of help from the text book and the following sites:\n",
    "\n",
    "* <a href=\"http://ravikiranj.net/posts/2012/code/how-build-twitter-sentiment-analyzer/\">how to build a twitter sentiment analyzer?</a>\n",
    "* <a href=\"http://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/\">Text Classification For Sentiment Analysis – Stopwords And Collocations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment\n",
    "\n",
    "Let us start with setting up the environment with all necessary libraries in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, math, collections, itertools, os\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Evaluator\n",
    "\n",
    "As we have learnt during our course, “features” are an important piece in sentiment analysis, whatever someone is analyzing in an attempt to correlate to the labels. In this code, the features will be the words in each review.\n",
    "\n",
    "For building feature corpus, I am going to use *sentence polarity dataset v1.0* having 5,331 positive and 5,331 negative processed sentences / snippets introduced by Cornell professor Bo Pang in Pang/Lee ACL 2005. Released July 2005. Though this data collected on movie review, but we can still use this dataset to use in our purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_features(feature_fun):\n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "    # http://stackoverflow.com/questions/367155/splitting-a-string-into-words-and-punctuation\n",
    "    # Breaks up the sentences into lists of individual words (as selected by the \n",
    "    # input mechanism) and appends 'pos' or 'neg' after each list\n",
    "    with open('./data/rt-polarity.pos', 'r') as posSentences:\n",
    "        for i in posSentences:\n",
    "            posWords = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "            posWords = [feature_fun(posWords), 'pos']\n",
    "            posFeatures.append(posWords)\n",
    "    with open('./data/rt-polarity.neg', 'r') as negSentences:\n",
    "        for i in negSentences:\n",
    "            negWords = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "            negWords = [feature_fun(negWords), 'neg']\n",
    "            negFeatures.append(negWords)\n",
    "\n",
    "    # Now we need to split the data into 80:20 ratio as training and \n",
    "    # testing data for a Naive Bayes classifier.\n",
    "    posCutoff = int(math.floor(len(posFeatures)*0.8))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*0.8))\n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "\n",
    "    # Trains a Naive Bayes Classifier using NLTK\n",
    "    classifier = NaiveBayesClassifier.train(trainFeatures)\n",
    "\n",
    "    #initiates referenceSets and testSets\n",
    "    referenceSets = collections.defaultdict(set)\n",
    "    testSets = collections.defaultdict(set)\n",
    "\n",
    "    # Puts correctly labeled sentences in referenceSets and the \n",
    "    # predictively labeled version in testsets\n",
    "    for i, (features, label) in enumerate(testFeatures):\n",
    "        referenceSets[label].add(i)\n",
    "        predicted = classifier.classify(features)\n",
    "        testSets[predicted].add(i)\n",
    "\n",
    "    print 'train on {:,} instances, test on {:,} instances'.format( \\\n",
    "                            len(trainFeatures), len(testFeatures))\n",
    "    print 'accuracy:', nltk.classify.util.accuracy(classifier, testFeatures)\n",
    "    print 'pos precision:', nltk.metrics.precision(referenceSets['pos'], \\\n",
    "                            testSets['pos'])\n",
    "    print 'pos recall:', nltk.metrics.recall(referenceSets['pos'], \\\n",
    "                            testSets['pos'])\n",
    "    print 'neg precision:', nltk.metrics.precision(referenceSets['neg'], \\\n",
    "                            testSets['neg'])\n",
    "    print 'neg recall:', nltk.metrics.recall(referenceSets['neg'], \\\n",
    "                            testSets['neg'])\n",
    "    print\n",
    "    classifier.show_most_informative_features(10)\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create a feature selection mechanism that uses all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bag of words feature selection:\n",
      "\n",
      "train on 8,528 instances, test on 2,134 instances\n",
      "accuracy: 0.778819119025\n",
      "pos precision: 0.787996127783\n",
      "pos recall: 0.762886597938\n",
      "neg precision: 0.770208900999\n",
      "neg recall: 0.794751640112\n",
      "\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     18.3 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "                   flaws = True              pos : neg    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "                 generic = True              neg : pos    =     13.0 : 1.0\n",
      "                  boring = True              neg : pos    =     12.4 : 1.0\n",
      "              refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "                    flat = True              neg : pos    =     11.8 : 1.0\n",
      "                 triumph = True              pos : neg    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "def make_complete_dict(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "print 'Using bag of words feature selection:\\n'\n",
    "monogramClassifier = evaluate_features(make_complete_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy 77.88% is good, still we will try to find a better accuracy. The precisions and recalls are also pretty close to each other indicating that it is classifying everything fairly evenly. Then we see the most informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopword Filtering**\n",
    "\n",
    "Let us now remove stopwords and see the accuracy parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 8,528 instances, test on 2,134 instances\n",
      "accuracy: 0.77038425492\n",
      "pos precision: 0.766882516189\n",
      "pos recall: 0.77694470478\n",
      "neg precision: 0.773979107312\n",
      "neg recall: 0.763823805061\n",
      "\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     18.3 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "                   flaws = True              pos : neg    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "                 generic = True              neg : pos    =     13.0 : 1.0\n",
      "                  boring = True              neg : pos    =     12.4 : 1.0\n",
      "              refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "                    flat = True              neg : pos    =     11.8 : 1.0\n",
      "                 triumph = True              pos : neg    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    " \n",
    "def stopword_filtered_word_feats(words):\n",
    "    return dict([(word, True) for word in words if word not in stopset])\n",
    " \n",
    "stopwordClassifier = evaluate_features(stopword_filtered_word_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy has gone down from 77.88% to 77.03%. Also negative recall has gone down a bit. This is an indication that stopwords add information to sentiment analysis classification. So, we should not remove stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigram Collection**\n",
    "\n",
    "Let us now include bigrams to see the accuracy parameters. We will use NLTK library bigram features for this. The BigramCollocationFinder maintains 2 internal FreqDists, one for individual word frequencies, another for bigram frequencies. Once it has these frequency distributions, it can score individual bigrams using a scoring function provided by BigramAssocMeasures, such chi-square. These scoring functions measure the collocation correlation of 2 words, basically whether the bigram occurs about as frequently as each individual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 8,528 instances, test on 2,134 instances\n",
      "accuracy: 0.787722586692\n",
      "pos precision: 0.792380952381\n",
      "pos recall: 0.779756326148\n",
      "neg precision: 0.783210332103\n",
      "neg recall: 0.795688847235\n",
      "\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     18.3 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "          (',', 'funny') = True              pos : neg    =     13.7 : 1.0\n",
      "                   flaws = True              pos : neg    =     13.7 : 1.0\n",
      "           ('dull', ',') = True              neg : pos    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "          ('to', 'care') = True              neg : pos    =     13.0 : 1.0\n",
      "           ('up', 'for') = True              pos : neg    =     13.0 : 1.0\n",
      "                 generic = True              neg : pos    =     13.0 : 1.0\n",
      "   ('examination', 'of') = True              pos : neg    =     13.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "def bigram_word_features(words, score_fn=BigramAssocMeasures.chi_sq, n=500):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    " \n",
    "bigramClassifier = evaluate_features(bigram_word_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "significant Accuracy is now up from 77.88% to 78.77%. Also both positive and negative precision and recall values have increased. So we can conclude that including bigrams can increase classifier effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get into the Business\n",
    "\n",
    "Now after seeing the benefits of bigram, let us start building our final model to find the sentiments of tweets. For this I am going to use some code snippets used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posFeatures = []\n",
    "negFeatures = []\n",
    "with open('./data/rt-polarity.pos', 'r') as posSentences:\n",
    "    for i in posSentences:\n",
    "        posWords = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "        posWords = [bigram_word_features(posWords), 'pos']\n",
    "        posFeatures.append(posWords)\n",
    "with open('./data/rt-polarity.neg', 'r') as negSentences:\n",
    "    for i in negSentences:\n",
    "        negWords = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "        negWords = [bigram_word_features(negWords), 'neg']\n",
    "        negFeatures.append(negWords)\n",
    "\n",
    "# Now we need to split the data into 80:20 ratio as training and \n",
    "# testing data for a Naive Bayes classifier.\n",
    "posCutoff = int(math.floor(len(posFeatures)*0.8))\n",
    "negCutoff = int(math.floor(len(negFeatures)*0.8))\n",
    "trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     18.3 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "          (',', 'funny') = True              pos : neg    =     13.7 : 1.0\n",
      "                   flaws = True              pos : neg    =     13.7 : 1.0\n",
      "           ('dull', ',') = True              neg : pos    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "          ('to', 'care') = True              neg : pos    =     13.0 : 1.0\n",
      "           ('up', 'for') = True              pos : neg    =     13.0 : 1.0\n",
      "                 generic = True              neg : pos    =     13.0 : 1.0\n",
      "   ('examination', 'of') = True              pos : neg    =     13.0 : 1.0\n",
      "                  boring = True              neg : pos    =     12.4 : 1.0\n",
      "              refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "        ('with', 'such') = True              pos : neg    =     12.3 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "                    flat = True              neg : pos    =     11.8 : 1.0\n",
      "                    lame = True              neg : pos    =     11.7 : 1.0\n",
      "                 triumph = True              pos : neg    =     11.7 : 1.0\n",
      "        ('the', 'title') = True              neg : pos    =     11.7 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n",
      "              disturbing = True              pos : neg    =     11.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Trains a Naive Bayes Classifier using NLTK\n",
    "classifier = NaiveBayesClassifier.train(trainFeatures)\n",
    "print classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     18.3 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "          (',', 'funny') = True              pos : neg    =     13.7 : 1.0\n",
      "                   flaws = True              pos : neg    =     13.7 : 1.0\n",
      "           ('dull', ',') = True              neg : pos    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "          ('to', 'care') = True              neg : pos    =     13.0 : 1.0\n",
      "           ('up', 'for') = True              pos : neg    =     13.0 : 1.0\n",
      "                 generic = True              neg : pos    =     13.0 : 1.0\n",
      "   ('examination', 'of') = True              pos : neg    =     13.0 : 1.0\n",
      "                  boring = True              neg : pos    =     12.4 : 1.0\n",
      "              refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "        ('with', 'such') = True              pos : neg    =     12.3 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "                    flat = True              neg : pos    =     11.8 : 1.0\n",
      "                    lame = True              neg : pos    =     11.7 : 1.0\n",
      "                 triumph = True              pos : neg    =     11.7 : 1.0\n",
      "        ('the', 'title') = True              neg : pos    =     11.7 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n",
      "              disturbing = True              pos : neg    =     11.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print bigramClassifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictSentiment(tweet, classifier):\n",
    "    twt = []\n",
    "    for key, value in tweet.iteritems():\n",
    "        twt.append(value)\n",
    "        \n",
    "    twFeatures = []\n",
    "    for i in twt[0]:\n",
    "        twWords = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "        twWords = [bigram_word_features(twWords), 'tbd']\n",
    "        twFeatures.append(twWords)\n",
    "\n",
    "    for i, (features, label) in enumerate(twFeatures):\n",
    "        predicted = classifier.classify(features)\n",
    "        print \"{} - {}\".format(twt[0][i], predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = {0: [\"you look so cool\", \"he looks not so cool\", \"so great\", \"not so great\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you look so cool - pos\n",
      "he looks not so cool - pos\n",
      "so great - pos\n",
      "not so great - pos\n"
     ]
    }
   ],
   "source": [
    "predictSentiment(test, stopwordClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you look so cool - pos\n",
      "he looks not so cool - neg\n",
      "so great - pos\n",
      "not so great - pos\n"
     ]
    }
   ],
   "source": [
    "predictSentiment(test, monogramClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you look so cool - pos\n",
      "he looks not so cool - neg\n",
      "so great - pos\n",
      "not so great - neg\n"
     ]
    }
   ],
   "source": [
    "predictSentiment(test, bigramClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data from Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to have data for analyzing its sentiment and I extract the data from twitter. Data extraction is based upon the key word and time period. I have choosen them just for demonstrating my project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse, urllib, urllib2, json, random\n",
    "import os, oauth2, datetime, re\n",
    "from datetime import timedelta\n",
    "\n",
    "class TwitterData:\n",
    "    def __init__(self):\n",
    "        self.currDate = datetime.datetime.now()\n",
    "        self.weekDates = []\n",
    "        self.weekDates.append(self.currDate.strftime(\"%Y-%m-%d\"))\n",
    "        for i in range(1,7):\n",
    "            dateDiff = timedelta(days=-i)\n",
    "            newDate = self.currDate + dateDiff\n",
    "            self.weekDates.append(newDate.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    def getTwitterData(self, keyword, time):\n",
    "        self.weekTweets = {}\n",
    "        if(time == 'lastweek'):\n",
    "            for i in range(0,6):\n",
    "                params = {'since': self.weekDates[i+1], 'until': \\\n",
    "                          self.weekDates[i]}\n",
    "                self.weekTweets[i] = self.getData(keyword, params)\n",
    "        elif(time == 'today'):\n",
    "            for i in range(0,1):\n",
    "                params = {'since': self.weekDates[i+1], 'until': \\\n",
    "                          self.weekDates[i]}\n",
    "                self.weekTweets[i] = self.getData(keyword, params)\n",
    "        return self.weekTweets\n",
    "    \n",
    "    def parse_config(self):\n",
    "        config = {}\n",
    "        if os.path.exists('config.json'):\n",
    "            with open('config.json') as f:\n",
    "                config.update(json.load(f))\n",
    "        return config\n",
    "\n",
    "    def oauth_req(self, url, http_method=\"GET\", post_body=None,\n",
    "                  http_headers=None):\n",
    "        config = self.parse_config()\n",
    "        consumer = oauth2.Consumer(key=config.get('consumer_key'), \\\n",
    "                                   secret=config.get('consumer_secret'))\n",
    "        token = oauth2.Token(key=config.get('access_token'), \\\n",
    "                             secret=config.get('access_token_secret'))\n",
    "        client = oauth2.Client(consumer, token)\n",
    "\n",
    "        resp, content = client.request(\n",
    "            url,\n",
    "            method=http_method,\n",
    "            body=post_body or '',\n",
    "            headers=http_headers\n",
    "        )\n",
    "        return content\n",
    "    \n",
    "    def getData(self, keyword, params = {}):\n",
    "        maxTweets = 50\n",
    "        url = 'https://api.twitter.com/1.1/search/tweets.json?'    \n",
    "        data = {'q': keyword, 'lang': 'en', 'result_type': 'recent', \\\n",
    "                'count': maxTweets, 'include_entities': 0}\n",
    "\n",
    "        if params:\n",
    "            for key, value in params.iteritems():\n",
    "                data[key] = value\n",
    "        \n",
    "        url += urllib.urlencode(data)\n",
    "        \n",
    "        response = self.oauth_req(url)\n",
    "        jsonData = json.loads(response)\n",
    "        \n",
    "        tweets = []\n",
    "        if 'errors' in jsonData:\n",
    "            print \"API Error\"\n",
    "            print jsonData['errors']\n",
    "        else:\n",
    "            for item in jsonData['statuses']:\n",
    "                tweets.append(item['text'])\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define keyword and time to extract data from twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter hash tag (with #) you want to retrieve? #trump\n",
      "#trump\n"
     ]
    }
   ],
   "source": [
    "keyword = raw_input('Enter hash tag (with #) you want to retrieve? ')\n",
    "print keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#keyword = '#Trump'\n",
    "time = 'today'\n",
    "twitterData = TwitterData()\n",
    "tweets = twitterData.getTwitterData(keyword, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.dump(tweets, open(\"./data/tweets.txt\",'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleanup data**\n",
    "\n",
    "* Convert the tweets to lower case\n",
    "* Remove unicode\n",
    "* Remove URLs\n",
    "* Remove @usernames\n",
    "* Remove additional white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processTweet(tweet):\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Remove unicode\n",
    "    tweet = tweet.encode('ascii','ignore')\n",
    "    #Remove www.* or https?://*\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',tweet)\n",
    "    tweet = tweet.replace(\"https://\",\"\")\n",
    "    tweet = tweet.replace(\"https:\",\"\")\n",
    "    #Remove @username\n",
    "    tweet = re.sub('@[^\\s]+',' ',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    #Finally remove rt from the begining \n",
    "    if tweet[:3] == \"rt \":\n",
    "        tweet = tweet[3:]\n",
    "    return tweet\n",
    "\n",
    "processedTweet = {k: map(processTweet, v) for k, v in tweets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(processedTweet, open(\"./data/processedTweets.txt\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am getting tired of the false trump attacks. this sums up liberals vs nazis scary but true! liberallogic greta ht - neg\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - neg\n",
      " isn't it time to remove donald trump from the hall of fame considering the racist rule and all? wwe trump - neg\n",
      " donald trump live in iowa buildthewall trumpkin trumptrain trumptoday trumpmiami makeamericagreatagain trump - pos\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - neg\n",
      "nice try cont. new leftist anti-trump narrative they \"care\" abt gop \"dilemma\". spare the false \"mournful state\" pity. cnn trump2016 - pos\n",
      "rt pat buchanan's message to america! get behind trump!!! makeamericagreatagain  - pos\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - neg\n",
      "i am getting tired of the false trump attacks. this sums up liberals vs nazis scary but true! liberallogic greta ht - neg\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - neg\n",
      "intelligent analysis of trump + prospects for 2016 repub convention by for 4 min.  - neg\n",
      "oh boy: hoist on his own petard!! this should be fun! anonymous just declared war against donald trump  - neg\n",
      "even die-hard hillaryclinton supporters can't think it's ok for trump to get more coverage than all the democrats combined! democracy - neg\n",
      "left targets trump by finding nutty supporters--let's meet some obama &amp; hillary supporters  - neg\n",
      "someone didn't like a trump campaign advertisement in cody, wyoming  - neg\n",
      "anonyops: optrump: petition to ban trump from the uk now has over half a million signatures!  - neg\n",
      "to hillary clinton, donald trump is no longer a laughing matter trump will beat her pants suit  - neg\n",
      "sharpton has done zero for blacks and only enriched himself. now he's enraged black ministers are warming to trump  - neg\n",
      "left targets trump by finding nutty supporters--let's meet some obama &amp; hillary supporters  - neg\n",
      "i typically vote for \"none of the above\", &amp; antiestablishment &amp; agenda. &amp; right now trump is it! imvotingtrump2016 ht - pos\n",
      "left targets trump by finding nutty supporters--let's meet some obama &amp; hillary supporters  - neg\n",
      "why dems and republicans are slamming trump?  - neg\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - neg\n",
      "why dems and republicans are slamming trump?  - neg\n",
      "we gave housesenate now they conspire to sabotage the will of the people's vote when trump wins the nomination?  - neg\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - neg\n",
      "that should make you stop talking abt trump and focus on true evil  - neg\n",
      "wow! trump supporters fire back at don't mess with us! trump2016 greta h - neg\n",
      "palin blasts msm herd, naive pundits for opposing trumps temp muslim ban radicalislam  - neg\n",
      "trump doesn't mind rich muslims...  - neg\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - neg\n",
      "or maybe twitter should make this punk famous trump  - pos\n",
      "this is what i have felt about how media is handling trump anti muslim rants. bad news!  - neg\n",
      "this is unexpected. moderate texas imam says he supports trump's proposal; then forced to resign from his mosque. http - neg\n",
      "palin blasts msm herd, naive pundits for opposing trumps temp muslim ban radicalislam  - neg\n",
      "trump remarks 2 banmuslims r causin white zionazis 2 attack us in usa may a white zionazi return the favor on trump bantrump 2 speak - neg\n",
      "trump donaldchimp (apology to chimps)  - pos\n",
      " donald trump live in iowa trump trumpdemands trump2016 women4trump needtrumpnow unitedwestand - pos\n",
      "i have hope now for this country! hollywood/media continue to lie about trump yet the people aren't listening anymore!  - neg\n",
      "why i love america: because men like this make the world a better place, and this is who we are. trump cruz  - pos\n",
      " so any trump supporter could have this little coward arrested for making a deaththreat, correct?  - neg\n",
      "specialreport the gope is way off like always! trump is going to win.......  - neg\n",
      "i've never been so disgraced &amp; disappointed w our govt &amp; our worlds in crisis &amp; all they care abt is taking dwn t - neg\n",
      "left targets trump by finding nutty supporters--let's meet some obama &amp; hillary supporters  - neg\n",
      "is the republican convention going to be open carry? gop trump gopwar - neg\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - neg\n",
      "left targets trump by finding nutty supporters--let's meet some obama &amp; hillary supporters  - neg\n",
      "franklin graham behind trump on muslim ban trumpisright bit me gop i support tedcruz islamistheproblem  - neg\n",
      "so any trump supporter could have this little coward arrested for making a death threat, correct?  - neg\n",
      "this is unexpected. moderate texas imam says he supports trump's proposal; then forced to resign from his mosque. http - neg\n"
     ]
    }
   ],
   "source": [
    "predictSentiment(processedTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
